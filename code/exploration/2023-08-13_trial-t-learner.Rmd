---
title: "T-learner test run"
author: '`r Sys.getenv("USER")`'
date: '`r format(Sys.time(), "%d %B, %Y")`' 
always_allow_html: true
output: 
  github_document:
    keep_html: true
---
	
```{r setup, include = FALSE}
file_name <- rstudioapi::getSourceEditorContext()$path

knitr::opts_chunk$set(
  fig.path =
    paste0("figures/", sub("\\.Rmd$", "", basename(file_name)), "/", sep = "")
)

ggplot2::theme_set(ggplot2::theme_classic(base_size = 10))
```

Today I'm going to quickly re-build the s-learner I made [here](2023-07-26_trial-s-learner.md.md) because it was rubbish and I realised I had left out the `alturism` variable. Then I'll have a go at building a t-learner, and then compare them!

```{r packages, message=FALSE, warning=FALSE}
library("tidyverse")
library("tidymodels")
library("vip")
library("here")
library("janitor")
library("patchwork")

set.seed(123)
```

```{r}
raw_data <- read_csv(here::here("data", "raw", "soga_gaston_data.csv"))

glimpse(raw_data)
```

## Refactoring the data

```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}

raw_data %>% 
  clean_names() %>%
  rename(childhood_nature = frequency_of_nature_experiences_childhood, 
         urban = degree_of_urbanisation,
         altruism = altruistic_value_orientation,
         pro_biodiversity = probability_of_adopting_pro_biodiversity_behaviour_number_3,
         recent_nature = frequency_of_nature_experiences_recent) %>% 
  select(-contains("probability")) %>% 
  mutate(recent_nature = case_when(recent_nature == 1 |
                                     recent_nature == 2 ~ 0,
                                   .default = 1) ) %>% 
  mutate(across(c(childhood_nature, urban, income, education_level),
                as.ordered)) %>% 
  mutate(across(c(recent_nature, pro_biodiversity, gender),
                as.factor)) %>% 
  
  drop_na() %>% 
  mutate(id = paste0("A", row_number(), sep = "")) %>% 
  select(id, pro_biodiversity, recent_nature, income, childhood_nature, 
         gender, education_level, urban, altruism) -> tidy_data
  
glimpse(tidy_data)  

```

## S-learner

### train

```{r}
data_split <- initial_split(tidy_data, prop = 1/3)
train_data_s <- training(data_split)
test_data_s <- testing(data_split)

rand_forest(
  mode = "classification",
  engine = "ranger"
) -> rf_def

rf_def %>% 
  fit(
    pro_biodiversity ~ recent_nature + 
      income + childhood_nature  + gender  + education_level + urban + altruism,
    data = train_data_s
  ) -> s_train_fit

s_train_fit

```

### test

```{r}
predict(s_train_fit, test_data_s, type = "prob") %>% 
  bind_cols(test_data_s) -> s_result

glimpse(s_result)

s_result %>% 
  roc_curve(pro_biodiversity, .pred_0) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal()
```

### tune

```{r}
rf_tune <- rand_forest(mtry = tune(), min_n = tune()) %>%
  set_engine("ranger", num.threads = 3) %>%
  set_mode("classification")

rf_tune

```

```{r}
finalize(mtry(), select(train_data_s, -id, - pro_biodiversity))

tree_grid <- grid_regular(mtry(c(1, 6)),
                          min_n(),
                          levels = 5)
```

Run models with range of chosen hyperparameters

```{r}
rf_recipe <- recipe(data = train_data_s,
                    formula = pro_biodiversity ~ recent_nature +
                      income + childhood_nature  + gender  + 
                      education_level + urban + altruism)

rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_tune)

# create a set of cross-validation resamples to use for tuning
trees_folds <- vfold_cv(train_data_s, v = 25)

rf_tune_res <- 
  tune_grid(rf_workflow,
            resamples = trees_folds,
            grid = tree_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

```


```{r}
rf_tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, linewidth = 1.5) +
  geom_point() +
  labs(y = "AUC")
```

Our best model has `min_n` of `r select_best(rf_tune_res, "roc_auc")$min_n` and `mtry` of `r select_best(rf_tune_res, "roc_auc")$mtry`. 

```{r}
best_auc <- select_best(rf_tune_res, "roc_auc")

final_rf <- finalize_model(
  rf_tune,
  best_auc
)

final_rf

```

Check out variable importance

```{r}
final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(pro_biodiversity ~ .,
    data = train_data_s %>% select(-id)
  ) %>%
  vip(geom = "point")
```

Alturism is pretty high!

`last_fit` fits a final model on the entire training set and evaluates on the testing set.

```{r}
final_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(final_rf)

final_res <- final_wf %>%
  last_fit(data_split)

final_res %>%
  collect_metrics()
```

```{r}
final_res$.predictions %>% 
  as.data.frame() %>% 
  roc_curve(pro_biodiversity, .pred_0) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal()
```

OK still bad! Let's build a t-learner!

## T-learner

1. separate out the with/without treatment groups from the training data and train 2 models
2. then use each of those models to predict the outcome for the full test dataset
3. the ITE is prediction with treatment - prediction without treatment

```{r}
train_1 <- train_data_s %>%
  filter(recent_nature == 1)

train_0 <- train_data_s %>%
  filter(recent_nature == 0)
```

### treatment == 1

Rather than fitting an initial model this time, I'll go straight into tuning hyperparameters.

```{r}
rf_recipe <- recipe(data = train_1,
                    formula = pro_biodiversity ~
                      income + childhood_nature  + gender  + 
                      education_level + urban + altruism)

rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_tune)

# create a set of cross-validation resamples to use for tuning
trees_folds <- vfold_cv(train_data_s, v = 25)

rf_tune_res_t1 <- 
  tune_grid(rf_workflow,
            resamples = trees_folds,
            grid = tree_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

```

```{r}
best_auc_t1 <- select_best(rf_tune_res_t1, "roc_auc")

final_rf_t1 <- finalize_model(
  rf_tune,
  best_auc_t1
)

final_rf_t1

```

Choose the model with the best hyperparameters and fit it on the test data where treatment == 1

```{r}
wf_t1 <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(final_rf_t1)

t1_fit <- fit(wf_t1, train_1)

```

### treatment == 0

Tune

```{r}
rf_recipe <- recipe(data = train_0,
                    formula = pro_biodiversity ~ 
                      income + childhood_nature  + gender  + 
                      education_level + urban + altruism)

rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_tune)

# create a set of cross-validation resamples to use for tuning
trees_folds <- vfold_cv(train_data_s, v = 25)

rf_tune_res_t0 <- 
  tune_grid(rf_workflow,
            resamples = trees_folds,
            grid = tree_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

```

```{r}
best_auc_t0 <- select_best(rf_tune_res_t0, "roc_auc")

final_rf_t0 <- finalize_model(
  rf_tune,
  best_auc_t0
)

final_rf_t0

```
Choose the model with the best hyperparameters and fit it on the test data where treatment == 0

```{r}
wf_t0 <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(final_rf_t0)

t0_fit <- fit(wf_t0, train_0)
```

## Fit t-learner

Now that we have 2 models, one trained on 0 data and one trained on 1 data. Next we need to make predictions for the test dataset using each of the models

```{r}
predict(t0_fit, test_data_s) %>% 
  cbind(test_data_s) %>% 
  mutate(.pred_class = 
           as.numeric(levels(.pred_class))[.pred_class]) -> tidy_0_pred

predict(t1_fit, test_data_s) %>% 
  cbind(test_data_s) %>% 
  mutate(.pred_class = 
           as.numeric(levels(.pred_class))[.pred_class]) -> tidy_1_pred

tidy_1_pred %>% 
  mutate(tlearn_ite = .pred_class - tidy_0_pred$.pred_class) %>% 
  select(- .pred_class) -> t_preds

glimpse(t_preds)
```

```{r}
t_preds %>% 
  roc_curve(truth = pro_biodiversity, tlearn_ite) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_path() +
  geom_abline(lty = 3) + 
  coord_equal()
```

Hmm this is worse than the s-learner, unless I've done something wrong.
