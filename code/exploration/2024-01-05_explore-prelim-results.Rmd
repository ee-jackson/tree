---
title: "Fit the meta-meta model and explore preliminary results"
author: '`r Sys.getenv("USER")`'
date: '`r format(Sys.time(), "%d %B, %Y")`' 
always_allow_html: true
output: 
  github_document:
    keep_html: true
---
	
```{r setup, include = FALSE}
file_name <- rstudioapi::getSourceEditorContext()$path

knitr::opts_chunk$set(
  fig.path =
    paste0("figures/", sub("\\.Rmd$", "", basename(file_name)), "/", sep = "")
)

ggplot2::theme_set(ggplot2::theme_classic(base_size = 10))
```

```{r packages, message=FALSE, warning=FALSE}
library("tidyverse")
library("here")
library("tidymodels")
library("vip")

set.seed(123)

```

Import the median ITE prediction errors 
(generated by [get-ite-predictions.R](/code/scripts/get-ite-predictions.R))

```{r}
ml_results <- readRDS(here("data", "derived", "results.rds"))

glimpse(ml_results)
```

I'm just going to do a quick glm for curiosity's sake.

```{r}
glm(median_error ~ learner + 
      n_train + 
      proportion_not_treated +
      assignment +
      var_omit, 
    ml_results, family = gaussian) -> glm_out

summary(glm_out)
```

## Fit random forest

### train test split 

```{r}
data_split <- initial_split(ml_results, prop = 1/3)
train_data <- training(data_split)
test_data <- testing(data_split)
```

### tune hyperparameters

```{r}
rf_tune <- rand_forest(mtry = tune(), min_n = tune()) %>%
  set_engine("ranger", num.threads = 3) %>%
  set_mode("regression")

tree_grid <- grid_regular(mtry(c(1, 5)),
                          min_n(),
                          levels = 5)

rf_recipe <- recipe(median_error ~ 
                      assignment + proportion_not_treated + 
                      n_train + learner + var_omit,
    data = train_data)

rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_tune)

# create a set of cross-validation resamples to use for tuning
trees_folds <- vfold_cv(train_data, v = 25)

rf_tune_res <- 
  tune_grid(rf_workflow,
            resamples = trees_folds,
            grid = tree_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

best_auc <- select_best(rf_tune_res, "rmse")

final_rf <- finalize_model(
  rf_tune,
  best_auc
)

final_rf
```

### vip

```{r}
final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(median_error ~ .,
    data = ml_results %>% select(-test_id)
  ) %>%
  vip(geom = "point")
```

### fit model

```{r}
final_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(final_rf)

final_res <- final_wf %>%
  last_fit(data_split)

final_res %>%
  collect_metrics()

# final_res %>%
#   collect_predictions()
```

## Visualise!

```{r}
plot_panels <- function(meta_learner, x_var, data) {
  data %>% 
    filter(learner == meta_learner) %>% 
    ggplot(aes(y = .data$median_error, x = .data[[x_var]] )) +
    geom_jitter(width = 0.1, alpha = 0.5, size = 0.5) +
    ylim(-3, 6) +
    geom_hline(yintercept = 0, colour = "red", size = 0.1, linetype = 2) +
    theme_classic(base_size = 6)
}

keys <- expand.grid(
  meta_learner = c("s", "t", "x"),
  x_var = c("assignment", "proportion_not_treated", "n_train", "var_omit")
  ) %>% 
  mutate(x_var = as.character(x_var)) %>% 
  arrange(meta_learner)

purrr::pmap(list(meta_learner = keys$meta_learner,
          x_var = keys$x_var), plot_panels, data = ml_results) -> plot_list

patchwork::wrap_plots(plot_list)

```

First row is s, then t, then x.
