---
title: "Baby's first S-learner"
author: '`r Sys.getenv("USER")`'
date: '`r format(Sys.time(), "%d %B, %Y")`' 
always_allow_html: true
output: 
  github_document:
    keep_html: true
---
	
```{r setup, include = FALSE}
file_name <- rstudioapi::getSourceEditorContext()$path

knitr::opts_chunk$set(
  fig.path =
    paste0("figures/", sub("\\.Rmd$", "", basename(file_name)), "/", sep = "")
)

ggplot2::theme_set(ggplot2::theme_classic(base_size = 10))
```

Today I'm going to try and get myself more familiar with random forests and S-learners by trying to analyse the data from [Soga & Gaston (2023)](https://doi.org/10.1111/conl.12945) using these methods.

Becks has done this before so I can use her script as a guide.

```{r packages, message=FALSE, warning=FALSE}
library("tidyverse")
library("tidymodels")
library("here")
library("janitor")
library("patchwork")

set.seed(123)
```

```{r}
raw_data <- read_csv(here::here("data", "raw", "soga_gaston_data.csv"))

glimpse(raw_data)
```

## Refactoring

The first thing that Becks does is re-factor the data. 
I think this is because some of the variables are continuous (i.e. how often did the person go out into nature) whereas we need a binary treatment effect (e.g. did you go outside, yes or no?).

```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
# 1. clean up the column names
# 2. remove probability columns 
# 3. make columns factors
# 4. make recent nature experiences into a binary treatment effect

raw_data %>% 
  clean_names() %>%
  rename(childhood_nature = frequency_of_nature_experiences_childhood, 
         urban = degree_of_urbanisation,
         altruism = altruistic_value_orientation,
         pro_biodiversity = probability_of_adopting_pro_biodiversity_behaviour_number_3,
         recent_nature = frequency_of_nature_experiences_recent) %>% 
  select(-contains("probability")) %>% 
  mutate(recent_nature = case_when(recent_nature == 1 |
                                     recent_nature == 2 ~ 0,
                                   .default = 1) ) %>% 
  mutate(across(- c(age, altruism), as.factor)) %>% 
  
  drop_na() %>% 
  mutate(id = paste0("A", row_number(), sep = "")) %>% 
  select(id, pro_biodiversity, recent_nature, income, childhood_nature, 
         gender, education_level, urban) -> tidy_data
  
glimpse(tidy_data)  

```

`pro_biodiversity` is our outcome and `recent_nature` is our treatment.

## S-learner

The S-learner is a single machine learning model. 
We will include the treatment as a feature in the model that tries to predict the outcome Y.

Becks uses `ranger::ranger()`, but I think we can use `parsnip::rand_forest()` and specify `set_engine("ranger")` to do the same thing but get tidy output.

### train

Let's use `initial_split` to select a random 3rd of the data to train the model.

```{r}
data_split <- initial_split(tidy_data, prop = 1/3)
train_data_s <- training(data_split)
test_data_s <- testing(data_split)

rand_forest(
  mode = "classification",
  engine = "ranger"
) -> rf_def

rf_def %>% 
  fit(
    pro_biodiversity ~ recent_nature + 
      income + childhood_nature  + gender  + education_level + urban,
    data = train_data_s
  ) -> s_train_fit

s_train_fit

```

### test

Run it on the test data to see if it's any good

```{r}
predict(s_train_fit, test_data_s, type = "prob") %>% 
  bind_cols(test_data_s) -> s_result

glimpse(s_result)

s_result %>% 
  roc_curve(pro_biodiversity, .pred_0) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal()
```

The ROC curve (receiver operating characteristic curve) tells us how good the model is.

Some notes from [wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic):

We have a binary classification problem, `pro_biodiversity` is either `1` or `0`. 
There are four possible outcomes from a binary classifier. 
If the outcome from a prediction is `1` and the actual value is also `1`, 
then it is called a true positive; 
however if the actual value is `0` then it is said to be a false positive. 
Conversely, a true negative has occurred when both the prediction outcome and the actual value are `0`, 
and false negative is when the prediction outcome is n while the actual value is `1`.

The ROC uses the false positive rate and true positive rate as x and y axes respectively.
The best possible prediction method would yield a point in the upper left corner of the ROC space, representing 100% sensitivity (no false negatives) and 100% specificity (no false positives).
A random guess would give a point along the dotted diagonal line.

So our current model is making predictions that are better than just taking a random guess, but not much better...
Perhaps this is because we need to tune the hyper parameters? 

## Tune hyperparameters

Let's try again with the help of [this `tidymodels` example](https://www.tidymodels.org/start/case-study/#second-model) 
and [this blog post](https://juliasilge.com/blog/sf-trees-random-tuning/).

They use the `tune` package (which is part of `tidymodels`) to tune hyperparameters.

We will try tuning `mtry` and `min_n`.

The `mtry` hyperparameter sets the number of predictor variables that each node in the decision tree “sees” and can learn about, 
so it can range from 1 to the total number of features present; 
when mtry = all possible features, the model is the same as bagging decision trees. 
The `min_n` hyperparameter sets the minimum n to split at any node.

First we define the model using `tune()` for the hyperparameters we want to tune.

```{r}
rf_tune <- rand_forest(mtry = tune(), min_n = tune()) %>%
  set_engine("ranger", num.threads = 3) %>%
  set_mode("classification")

rf_tune

```

We can train many models using resampled data and see which models turn out best. 
We need to create a regular grid of values to try using some convenience functions for each hyperparameter.

`dials::grid_regular()` chooses sensible values to try for each hyperparameter.

```{r}
finalize(mtry(), select(train_data_s, -id, - pro_biodiversity))

tree_grid <- grid_regular(mtry(c(1, 6)),
                          min_n(),
                          levels = 5)

glimpse(tree_grid)
```

Those are the hyperparameters for the 25 models we'll be running.

```{r}
rf_recipe <- recipe(data = train_data_s,
                    formula = pro_biodiversity ~ recent_nature +
                      income + childhood_nature  + gender  + 
                      education_level + urban)

rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_tune)

# create a set of cross-validation resamples to use for tuning
trees_folds <- vfold_cv(train_data_s, v = 25)

rf_tune_res <- 
  tune_grid(rf_workflow,
            resamples = trees_folds,
            grid = tree_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

```

How did it turn out?

`roc_auc()` is a metric that computes the area under the ROC curve.
Higher AUC = good

```{r}
rf_tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, linewidth = 1.5) +
  geom_point() +
  labs(y = "AUC")
```

So it looks our best model has `min_n` of `r select_best(rf_tune_res, "roc_auc")$min_n` and `mtry` of `r select_best(rf_tune_res, "roc_auc")$mtry`. 

Let's select the best model and take a look at it.

```{r}
best_auc <- select_best(rf_tune_res, "roc_auc")

final_rf <- finalize_model(
  rf_tune,
  best_auc
)

final_rf

```

We can take a look at variable importance usng `vip`.

```{r}
library("vip")

final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(pro_biodiversity ~ .,
    data = train_data_s %>% select(-id)
  ) %>%
  vip(geom = "point")
```

`recent_nature` (our treatment effect) is the most important.

`last_fit` fits a final model on the entire training set and evaluates on the testing set.

```{r}
final_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(final_rf)

final_res <- final_wf %>%
  last_fit(data_split)

final_res %>%
  collect_metrics()
```

```{r}
final_res$.predictions %>% 
  as.data.frame() %>% 
  roc_curve(pro_biodiversity, .pred_0) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal()
```

Ah damn it's still bad ahah! Looks kind of the same honestly.
